---
title: "Generating Reproducible Statistical Analyses and Evaluation Reports:
        Principles, Practices, and Free Software Tools"
subtitle: "Demonstration at *Evaluation 2024: Amplifying and Empowering 
           Voices*, annual conference of the American Evaluation Association, 
           Portland, OR"
author: "Steven J. Pierce"
institute: "Center for Statistical Training and Consulting, Michigan State University"
date: October 22, 2024
title-slide-attributes: 
  data-notes: My talk today focuses on some principles, practices, and tools 
              that I use to generate reproducible statistical analyses and 
              evaluation reports. My goal is promote reproducibility and 
              encourage evaluators to build their capacity and competency by 
              adopting practices and tools for reproducible research. Hopefully, 
              sharing my experience and insights with you during this talk will 
              persuade you why that's beneficial, motivate you to do so, and 
              accelerate your learning journey. 
format: 
  revealjs:
    embed-resources: true
    theme: [default, CSTAT_theme.scss]
    slide-number: c/t
    show-notes: false
    header-logo: graphics/Combomark-Horiz_Green-RGB.png
    footer: <a href="https://github.com/sjpierce/Pierce.AEA2024">https://github.com/sjpierce/Pierce.AEA2024</a>
    logo: graphics/CSTAT_5x5_Transparent.png
    link-external-icon: true
    link-external-newwindow: true
filters:
  - reveal-header
---

# Section 1

## Reproducible Research (RR)

... is achieved when investigators *share all the materials required to exactly 
recreate the findings* so that others can verify them or conduct alternative 
analyses.

::: {.notes}
So, let’s set the stage by defining RR, which is research for which the
investigators have shared all the materials required to exactly recreate the
findings so that others can verify them or conduct alternative analyses. While 
there are differences between research and evaluation writ broadly, I believe 
reproducbility is just as valuable in--and applicable to--evaluation work as it 
is in research contexts.
:::

## RR is a Product

```{dot}
//| fig-width: 9
//| fig-height: 4
digraph G {
rankdir="TB";

node [shape = "box", style= "filled", fontname="Lato", fillcolor = "#C3FFEC"]
Quant [label = "Quantitative Methods"]
Qual [label = "Qualitative Methods"]
Mixed [label = "Mixed Methods"]
RR [label = "Reproducible Research"]

Quant -> Mixed -> RR
Quant -> RR
Qual -> Mixed
Qual -> RR
}
```

RR is a product of *how we work*, not which methods we use. 

::: {.notes}
RR is a product of the research process that’s presumably independent of whether 
we use quantitative, qualitative, or mixed methods. What matters is how we work 
and share the materials. However, I suspect it’s easier to apply RR principles 
to quantitative methods and those are my forte as a statistician, so I'll focus 
on that route to reproducibility today. 
:::

## Why RR? {background-image="graphics/Woman_Raising_Hand.jpg" background-size="contain" background-position="right" background-repeat="no-repeat"}

:::: {.columns}

::: {.column width="45%"}
Several forces are promoting and enabling the push toward reproducibility.
:::

::::

::: {.notes}
So, why RR? There are many reasons to work toward meeting the RR standard. I’ll 
just touch on a few key forces that are promoting and enabling the push toward 
it.
:::

## Requirements for Reproducibility

* Data
* Code
* Software 

::: {.notes}
Speaker notes go here.
:::

## Benefits of Reproducibility

* Accuracy
* Efficiency
* Credibility

::: {.notes}
Speaker notes go here.
:::

## Guiding Principles

:::: {.columns}

::: {.column width="50%"}
* Collaboration
* Automation
* Organization
:::

::: {.column width="50%"}
* Preservation
* Integration
* Separation
:::

::::

::: {.notes}
Understanding a few guiding principles helps people figure out how to do RR and
why it may be worth using the more complex methods of achieving it. Here are the
principles I’ve identified so far. Thinking about reproducibility as
collaboration encourages us to prepare and share materials that will make our 
work accessible to others. Automation helps us achieve efficiency and accuracy 
because scripts can be faster and more reliable than manual processes. 
Organization simplifies the task of communicating to others how to reproduce
our work. The preservation principle guides us to keep our raw data pristine and
to keep a good audit trail. The integration principle shows up when we build 
documentation directly into data files, put comments in code, and use code to 
directly insert statistical results into reports. The separation principle helps
us manage complexity by parsing a project into manageable pieces; for example
separating data management from analysis, or keeping raw data separate from
working data files used for specific analyses. Keep an eye out for instances of
these principles in action throughout the rest of this talk. My examples will
aim toward the middle of the complexity spectrum.
:::

## Manual Workflow
* Use graphical user interface for software instead of coding scripts
* Manually updating data files 
* Version control through saving to new file names
* Disorganized folders & files 
* Copy & paste output from stats software to word processor

::: {.notes}
Speaker notes go here.
:::

## Reproducible Workflow

* Eliminate manual steps!
* Produce fully-formatted, publication-quality reports:
    * Narrative text, headings
    * Software code (optional)
    * Dynamically inserted statistical results
    * Dynamic tables
    * Dynamic figures 
    * Dynamic reference sections

::: {.notes}
Speaker notes go here.
:::

## Version Control 

::: {.notes}
Speaker notes go here.
:::

## Narrative Text

* Formatting
* Headings
* Links



## Inline R Code


## Tables


## Figures


## Reference Lists

* Citation style language file to control format
* BibTeX files containing data

## Parameterized Reports


```{dot}
digraph G {
rankdir="LR";
compound=true;

node [shape = "note"]
Quarto [label = "Document.qmd"]
BibTeX [label = "references.bib"]
CSL [label = "apa.csl"]

subgraph cluster_1 {
  label = "Document.pdf";
  labeljust=l;
  Refs [shape = "component", label = "References Section"]
}

Quarto -> Refs [label = "Render", lhead = "cluster_1"] 
BibTeX -> CSL [label = "Data"]
CSL -> Refs [label = "Format"] 
}
```


## References {.scrollable}

::: {#refs}
:::
