---
title: "Generating Reproducible Statistical Analyses and Evaluation Reports:
        Principles, Practices, and Free Software Tools"
subtitle: "Demonstration at *Evaluation 2024: Amplifying and Empowering 
           Voices*, annual conference of the American Evaluation Association, 
           Portland, OR"
author: "Steven J. Pierce"
institute: "Center for Statistical Training and Consulting, Michigan State University"
date: October 22, 2024
title-slide-attributes: 
  data-notes: My talk today focuses on some principles, practices, and tools 
              that I use to generate reproducible statistical analyses and 
              evaluation reports. My goal is promote reproducibility and 
              encourage evaluators to build their capacity and competency by 
              adopting practices and tools for reproducible research. Hopefully, 
              sharing my experience and insights with you during this talk will 
              persuade you why that's beneficial, motivate you to do so, and 
              accelerate your learning journey. My slides are in an HTML file 
              available from the link at the bottom of the screen. They have 
              many links to resources built right in and are themselves a 
              reproducible product. 
csl: apa-numeric-superscript-brackets.csl
bibliography: references.bib
format: 
  revealjs:
    embed-resources: true
    theme: [default, CSTAT_theme.scss]
    controls: true
    slide-number: c/t
    show-notes: false
    header-logo: graphics/Combomark-Horiz_Green-RGB.png
    footer: <a href="https://github.com/sjpierce/Pierce.AEA2024">https://github.com/sjpierce/Pierce.AEA2024</a>
    logo: graphics/CSTAT_5x5_Transparent.png
    link-external-icon: true
    link-external-newwindow: true
filters:
  - reveal-header
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library(knitr)
library(kableExtra) # Functions for formatting nice tables. 
```

## Outline
* What do you mean by reproducible? 
* Why should we aim for reproducibility?
* How do we achieve reproducibility?

::: {style="text-align: right;"}
# What do you mean by <br> reproducible?  {background-image="graphics/pexels-august-de-richelieu-4427508.jpg" background-size="contain" background-position="left" background-repeat="no-repeat"}

Let's define some concepts.
<br> <br> <br> <br> <br>
:::

## Reproducible Research (RR)
... is achieved when investigators *share all the materials required to exactly 
recreate the findings* so that others can verify them or conduct alternative 
analyses.

::: {.notes}
So, let’s set the stage by defining RR, which is research for which the
investigators have shared all the materials required to exactly recreate the
findings so that others can verify them or conduct alternative analyses. While 
there are differences between research and evaluation writ broadly, I believe 
reproducbility is just as valuable in--and applicable to--evaluation work as it 
is in research contexts.
:::

## Statistical Results Can Be:

:::: {.columns}

::: {.column width="35%"}
::: {.fragment}
[Repeatable]{.bg}

* Original analyst
* Original data
:::
:::

::: {.column width="35%"}
::: {.fragment}
[Reproducible]{.bg}

* New analyst
* Original data
:::
:::

::: {.column width="30%"}
::: {.fragment}
[Replicable]{.bg}

* New analyst
* New data
:::
:::

::::

::: {.notes}
Let's put reproducibility into a broader context. Statistical results can be 
classified into a few different categories. They are repeatable when the 
original analyst can reliably obtain the same results from the original data. 
They are reproducible if a new analyst can obtain the same result from the 
original data. Finally, they are replicable if a new analyst can obtain the same 
result from a new set of data. How confident are you about about where your work 
falls on this spectrum? 
:::

## RR is a Product

```{dot}
//| fig-width: 9
//| fig-height: 4
digraph RRProduct {
rankdir="TB";

node [shape = "box", style= "filled", fontname="Lato", fillcolor = "#C3FFEC"]
Quant [label = "Quantitative Methods"]
Qual [label = "Qualitative Methods"]
Mixed [label = "Mixed Methods"]
RR [label = "Reproducible Research"]

Quant -> Mixed -> RR
Quant -> RR
Qual -> Mixed
Qual -> RR
}
```

RR is a product of *how we work*, not which methods we use. 

::: {.notes}
RR is a product of the research process that’s presumably independent of whether 
we use quantitative, qualitative, or mixed methods. What matters is how we work 
and share the materials. However, I suspect it’s easier to apply RR principles 
to quantitative methods and those are my forte as a statistician, so I'll focus 
on that route to reproducibility today. 
:::

# Why aim for <br> reproducibility? {background-image="graphics/Woman_Raising_Hand.jpg" background-size="contain" background-position="right" background-repeat="no-repeat"}

:::: {.columns}

::: {.column width="45%"}
Several forces are promoting and enabling the push toward reproducibility.
:::

::::

::: {.notes}
So, why aim for RR? There are many reasons to work toward meeting the RR
standard. I’ll just touch on a few key forces that are promoting and enabling
the push toward it.
:::

## Methods Matter!

```{dot}
//| fig-width: 10
//| fig-height: 2
digraph Continuum {
rankdir="LR";

node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", fillcolor = "#C3FFEC"]
Low [label = "Low"]
High [label = "High"]

edge [fontname="Lato", fontsize="40pt", arrowsize = 1.5]
Low -> High [label = "Trustworthiness & Credibility", dir = "both", minlen = 9]
}
```

::: {.fragment}
[Irreproducible < Reproducible < Replicated]{.absolute left="15%" top="35%"}
:::

::: {.fragment}
:::{.callout-important}
Reproducibility is an attainable minimum standard for science [@Peng-RN3189].
:::
:::

::: {.notes}
Remember that our methods *matter*: how we conduct studies influences where our
results fall on a continuum of trustworthiness and scientific credibility.
Studies that use good scientific practices have higher trustworthiness and
credibility. Let's link this continuum to reproducibility. 

At the low end, we have IRR, which relies on investigator reputation to
establish credibility because key methodological details aren’t published and
the findings can’t be independently verified. Reviewers and readers have limited
ability to judge the quality of the research. IRR provides a poor foundation for
future replication studies.

RR findings can be independently verified from the original data, reducing
reliance on investigator reputation. Reviewers and readers can better judge
quality of research on its own merits. Furthermore, RR provides a solid
foundation for future replication studies by publishing key methodological
details.

Finally, replicated research has the highest level of credibility because
independent investigators have replicated the findings with new studies,
indicating that the probability of opportunistic biases is low and the findings
generalize beyond the original sample. This dramatically increases
trustworthiness of a finding.

Roger Peng has argued that RR is an attainable minimum standard for science
(Peng et al., 2006). 
:::

## Guiding Principles for Evaluators
Pursuing reproducibility enacts our guiding principles 
[@AEA-RN8648]: 

:::: {.columns}

::: {.column width="25%"}
![](graphics/AEA_Logo.png){fig-alt="AEA logo." width=150% height=150%}
:::

::: {.column width="75%"}

::: {.incremental}
* Systematic inquiry
* Competence 
* Integrity 
:::

:::

::::

::: {.notes}
Pursuing reproducibility is a good way to enact several of AEA's guiding
principles for evaluators, namely systematic inquiry, competence, and integrity.
:::

## Funders Value RR {.smaller}
[Data sharing and reproducibility initiatives]{style="font-size: 1.25em"}

:::: {.columns}

::: {.column width="15%"}

![](graphics/NIH_Logo.png){fig-alt="NIH logo." width=75% height=75%}
![](graphics/NSF_Logo.jpg){fig-alt="NSF logo."  width=75% height=75%}

![](graphics/IES+logo.png){fig-alt="IES logo." width=125% height=125%}
:::

::: {.column width="85%"} 

* [https://sharing.nih.gov/data-management-and-sharing-policy](https://sharing.nih.gov/data-management-and-sharing-policy)
* [https://grants.nih.gov/policy/reproducibility/index.htm](https://grants.nih.gov/policy/reproducibility/index.htm)
* [https://new.nsf.gov/funding/data-management-plan](https://new.nsf.gov/funding/data-management-plan)
* [https://www.nsf.gov/pubs/2019/nsf19022/nsf19022.pdf](https://www.nsf.gov/pubs/2019/nsf19022/nsf19022.pdf)
* [https://www.nsf.gov/attachments/134722/public/Reproducibility_NSFPlanforOMB_Dec31_2014.pdf](https://www.nsf.gov/attachments/134722/public/Reproducibility_NSFPlanforOMB_Dec31_2014.pdf)

* [https://ies.ed.gov/blogs/research/post/companion-guidelines-on-replication-and-reproducibility-in-education-research](https://ies.ed.gov/blogs/research/post/companion-guidelines-on-replication-and-reproducibility-in-education-research)

:::

::::

::: {.notes}
There are now plenty of signs that funders value RR. For example, NIH and NSF
have data sharing policies, so you ought be to addressing data sharing in your
grant proposals. They’ve also launched reproducibility initiatives. NIH started 
by promoting selected aspects of RR for preclinical studies, but has expanded 
that to a wider range of studies. Other federal funders like NSF and IES have 
also published materials promoting reproducibility. 
:::

## Emerging Scientific Norms
* Open science: 
    * [https://www.cos.io/open-science](https://www.cos.io/open-science) 
    * [https://open.science.gov](https://open.science.gov)
* Pre-registered methods [@Bosnjak-RN8375; @Decoster-RN3192; @Moore-RN3318]
* Reporting guidelines [@Appelbaum-RN3691; @Moher-RN2216; @Schulz-RN2218; @von-Elm-RN3288]
* Publishing raw data [@Hrynaszkiewicz-RN3087; @Hrynaszkiewicz-RN3088]
* Journal & publisher policies [@Decoster-RN3192; @Hrynaszkiewicz-RN3087; @Hrynaszkiewicz-RN3088; @Laine-RN3186; @Peng-RN3187]
* Institutional policies

::: {.notes}
There are emerging scientific norms aligned with RR. These are evident in the
open science movement, calls for pre registered methods/trials, reporting
guidelines, calls to publish raw data, and RR policies being adopted by various
journals and publishers. They are also reflected in some local institutional
policies.
:::

## Publishing Technology

```{dot}
//| fig-width: 10
//| fig-height: 5
digraph PubTech {
rankdir="TB"; splines=false; nodesep = .6;

node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", 
      fillcolor = "#C3FFEC", margin = .25]
Au [label = "Author", shape = "egg"]
Jo [label = "Journal"]
Ar [label = "Archive"]

subgraph cluster_0 {
style="rounded,dashed"; penwidth = 3; color = "black";
label = "Print\n Distribution"; labelloc=b;
fontname="Lato"; fontsize="40pt"; fontcolor = "black";
Pa [label = "Manuscript", shape = "note"]
}

subgraph cluster_1 {
style="rounded,dashed"; penwidth = 3; color = "red"; margin = .3;
label = "Online Distribution\nEasy, low cost, no page limits!"; labelloc=b;
fontname="Lato"; fontsize="40pt"; fontcolor = "red";
SF [label = "Manuscript &\nSupplemental\nFiles", shape = "note"]
CD [label = "Codebooks,\nData, &\nSoftware", shape = "note"]
}

edge [fontname="Lato", fontsize="40pt", arrowsize=1.5]
Jo -> Au [dir = "back"]
Au -> Ar 
Jo -> Pa 
Jo -> SF
Ar -> CD

//Jo -> Ar [style = "invis", minlen = 6.5]

rank = same; {Jo; Au; Ar;}
}
```

::: {.notes}
Modern publishing technology makes RR easier. When we we relied mostly on
printed journals to distribute scientific papers, cost and page limits
restricted the ability to do RR. Now, online publishing means you can easily
share supplemental files alongside your papers, or deposit data, codebooks and
other documentation, software files like analysis scripts, and even raw output
in data archives/repositories. This vastly reduces cost, eliminates page limits,
and increases opportunity to meet RR standards.
:::

## Career Benefits of RR
* Motivation to focus on quality
* Become more efficient
* Create more products
* Easier to get published 
* Get cited more often
* Build your reputation

::: {.notes}
There are some real career benefits of striving to create reproducible 
research and evaluation products. For example, it motivates you to be more
careful about avoiding errors, thereby increasing the quality of your work. 
It will make you more efficient and the process produces more products. 
It may be easier to get published--and to publish in better journals--and you 
might ultimately get cited more often as people either adopt your
methods, use your data for new studies, or discuss your work as an exemplar of
high quality science. That will all build your professional reputation.
:::

# How do we achieve reproducibility? {background-image="graphics/Hands_Raised.jpg" background-size="contain" background-position="bottom" background-repeat="no-repeat"}

Understand the criteria, then apply principles, practices, & tools. 
<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> 

::: {style="text-align: center;"}
## Criteria for Reproducbility <br> [@Peng-RN3189] {background-image="graphics/Woman_Thinking.png" background-size="contain" background-position="left" background-repeat="no-repeat"}

::: {.incremental}
* Data
* Methods
* Documentation
* Distribution
:::

::: {.notes}
Reproducibility is a continuum, but there are some simple criteria that we can
use to evaluate how reproducible a given project is. The data criterion requires
that you share the data you used, since nobody can recreate your findings
without them. The methods criterion requires that you share your methods in
sufficient detail, preferably through sharing the actual software code or
scripts you used to get the results. The documentation criterion requires that
you share adequate documentation of the data set, software environment, and
software code so others can understand what you had, what you did with it, and
what it really means. Finally, the distribution criterion requires that you use
standard methods to distribute all these materials. 
:::

:::

##  Materials Required to Recreate Findings

```{r}
#| label: tbl-materials-findings

tibble(Materials = c("Manuals & procedures", "Instruments & scoring rules", 
                     "Codebooks", "Methods applied", "Data mgt decisions", 
                     "Data files", "Software & analysis scripts"),
       Findings = c("Statistics", "Coefficients & p-values", 
                    "Confidence intervals", "Effect sizes", 
                    "Model fit indices", "Figures", "Tables")) %>% 
  kable(., format = "html") %>% 
  kable_material(c("striped"), html_font = "Lato")
```

::: {.notes}
So let’s go back and unpack the definition of RR by giving some examples of the
kinds of materials & findings we are discussing. This is just a sample of what 
could be encompassed by these two terms.
:::

## Principles for Achieving Reproducibility

:::: {.columns}

::: {.column width="50%"}
* Collaboration
* Organization
* Automation
:::

::: {.column width="50%"}
* Preservation
* Integration
* Separation
:::

::::

::: {.notes}
Understanding a few guiding principles helps people figure out how to do RR.
Here are the principles I’ve identified so far. Thinking about reproducibility
as collaboration encourages us to prepare and share materials that will make our
work accessible to others. Organization simplifies communicating to others how
to reproduce our work. Automation helps us achieve efficiency and accuracy
because scripts can be faster and more reliable than manual processes. The
preservation principle guides us to keep our raw data pristine and to keep a
good audit trail. The integration principle suggests building documentation
directly into data files, putting comments in code, and using code to directly
insert statistical results into reports. The separation principle helps us
manage complexity by splitting projects or tasks into pieces; for example
separating data management from analysis, or keeping raw data separate from
working data files used for specific analyses. Keep an eye out for instances of
these principles in action throughout the rest of this talk. My examples will
aim toward the middle of the complexity spectrum.
:::

## Workflow Woes
* Use GUI, menus, & dialog boxes to do tasks 
* Manually updating data files 
* Version control through saving to new file names 
* Disorganized folders & files 
* No audit trail 
* Copy & paste output from stats software to word processor 
* Fixing mistakes took lots of time 

::: {.notes}
As a student, I was taught a manual workflow for doing data analysis and writing
reports. We used the GUI, menus, and dialog boxes in SPSS to do data management
and analysis. We updated data files manually. We created tons of files, saving
to a new filename as changes accrued. That led to confusing proliferation of
folders and files with little planned structure. It was hard to tell who had
last edited various files and which versions to use. We retyped or pasted
results from SPSS output into Word documents then spent lots of time formatting
them. If we made a mistake, it could takes hours or even days to re-do things.
Over time I learned some strategies to improve that workflow, such as using SPSS
syntax files to automate tasks, adding comments in the syntax files, adopting
file naming conventions, and so on. But part of the problem was that the
software tools weren't integrated and lacked support for better workflows.
:::

## Spock {background-image="graphics/Spock.jpg" background-position="top" }

::: {.absolute left="0%" top="65%" style="font-size:.8em; padding: 0.25em .5em; background-color: rgba(255, 255, 255, .5); backdrop-filter: blur(5px); box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .5); border-radius: 20px;"}

"Captain, you're asking me to work with equipment which is hardly very far 
ahead of stone knives and bearskins."  
  
*Star Trek (1966) - S01E28 The City on the Edge of Forever*
:::

::: {.notes}
Technology has vastly improved since my student days, but many evaluators still 
use manual workflows like that. They don't know what is possible now. Given what 
I've learned in recent years, if I were asked to resume using those older tools 
and workflows, I'd probably quote Spock.
:::

## Free, Open Source Software Tools for Reproducible Research {background-image="graphics/Keyboard_Typing.jpg"}

* [R](https://www.r-project.org/) [@R-Devel-Core-RN8182], for statistical 
  computing
* [RStudio Desktop](https://posit.co/download/rstudio-desktop/) 
  [@RStudio-Team-RN8351], an editor
* [Quarto](https://quarto.org) [@Allaire-RN8427], for dynamic documents 
* [Git](https://git-scm.com/) [@Torvalds-RN3929], for version control
* [GitHub.com](https://github.com), for sharing & collaborating on code

::: {.notes}
Fortunately, there's a better way to do analyses and write reports. It depends
on open-source software and learning new workflows. These are my favorite tools
now. R is a fantastic option for managing and analyzing data and creating
figures. RStudio Desktop is an excellent editor for working with R that also
simplifies using Quarto, Git, and GitHub. Quarto supports creating dynamic
documents that render to a variety of output formats (HTML, PDF, Word,
PowerPoint, etc.), while Git and GitHub together support version control,
collaboration, and sharing materials. Collectively, they comprise an elegant,
integrated set of tools for doing reproducible analyses and reports.
:::

## Dynamic Documents Via Quarto + R

```{dot}
//| fig-width: 10
//| fig-height: 5
digraph DynDoc {
rankdir="LR";
compound=true;

node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", 
      fillcolor = "#C3FFEC", margin = .25]
Data [label = "Study_Data.csv\n(Raw Data File)"]
Quarto [label = "Report.qmd\n(Quarto Script\nw/ R code)"]
BibTeX [label = "references.bib\n(BibTeX Data File)"]
CSL [label = "apa.csl\n(Citation Style\nLanguage File)"]
Report [label = "Report.pdf\n(Formatted\nOutput)"]

edge [fontname="Lato", fontsize="30pt", arrowsize = 1.5]
Data -> Quarto [label = "Read by"]
BibTeX -> Quarto [label = "Cited in"]
CSL -> Quarto [label = "Used by"] 
Quarto -> Report [label = "Render"] 
}
```

::: {.notes}
Dynamic documents are a crucial technology for reproducibility. They implement
the concept of literate programming, where one embeds code directly in a
document such as a report, interleaving it with narrative text, headings, and
in-text citations. When you render dynamic documents, the code is executed and
the results are directly inserted into the output. That includes statistics
computed from the data, figures, tables, formatted equations, fully-formatted
reference sections, and more. This diagram shows the files involved for a simple
example. The idea is that rendering your report script produces an output such
as a PDF file ready for delivery to your client.

Several principles are in action here: Separation of data from code, integration
of R code into the Quarto script, where it automates data management and
analyses. Quarto automates inserting results into the report. The BibTeX file
contains data about references you cite in the report, while the CSL file tells
QAuarto how to format the citations and the reference list. I'm using similar
file names to stay organized: they show which script creates the report. Saving
all the files follows the preservation principle. One can rapidly reproduce the
report by re-rendering the script, or fix an error in the script then render
again to automate updating the report.
:::

## Research Compendium {background-image="graphics/pexels-anete-lusina-4792285.jpg"}

:::: {style="padding: 0.25em .5em; background-color: rgba(255, 255, 255, .5); backdrop-filter: blur(5px); box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .5); border-radius: 20px;"}
A compendium organizes digital files so others can review or use them to 
reproduce results, or do new analyses. [@Marwick-RN3899] It should: 

::: {.incremental}
* Use prevailing conventions to organize the project
* Separate data, method, & output but show how they are related
* Specify the software used in the original analysis
:::

::::

::: {.notes}
Dynamic documents are only part of the solution. Most projects require more than
couple files, so organizing your materials into a research compendium is useful.
A compendium is a structured folder containing all your reproducibility files.
An effective one should use prevailing conventions to organize the project; keep
data, method, and output separate even while revealing how they are related; and
it should tell users what software was used in the original analysis. You can 
facilitate collaboration by thoughtfully setting up a compendium with attention  
to applying the organization, separation, and integration principles. 
:::

## R Packages
... are folders of digital files designed for sharing code & help documentation. 
[@Wickham-RN3898] They

* Follow a conventional structure 
* Can contain data, meta-data, & other documentation 
* Can contain dynamic documents & rendered output 

::: {.fragment}
:::{.callout-tip}
Use an R package for a research compendium! [@Marwick-RN3899] 
:::
:::

::: {.notes}
R packages are normally used to share code & associated help files so other 
people can use custom software. 
:::

## Git Repositories
... are folders of files being tracked by Git [@Torvalds-RN3929] for version 
control purposes. [@Bryan-RN3900; @Chacon-RN3480] They:

* Preserve a history of changes to each tracked file
* Allow recovering a file's prior state from the history 
* Can be local or remote (hosted on a server) 
* Can be private or public 

::: {.fragment}
:::{.callout-tip}
Put your research compendium in a Git repository!
:::
:::

::: {.notes}
Git is software for doing version control on code and other files stored in
repositories, which are just folders of files. You can choose which files to
track, then log of all committed changes to them, when they were made, who made
them, and why. That preserves a fantastic audit trail. You can recover the exact
state of a file as it existed at any commit in the history. You can make either
private or public repositories. I usually keep a repository private until the
corresponding paper has been published, then I can make it public to enhance
reproducibility.
:::


## GitHub

```{dot}
//| fig-width: 10
//| fig-height: 5
digraph GitHub {
rankdir="TB"; nodesep = .25; splines = "ortho";

subgraph cluster_0 {
style="rounded,dashed"; penwidth = 3; color = "black";
label = "GitHub Server"; labelloc=t;
fontname="Lato"; fontsize="40pt"; fontcolor = "black";
node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", 
      fillcolor = "#C3FFEC", margin = .25]
RR [label = "OurProject\n(Remote Main Repository)"]
}

node [style = "invis"]
Hidden

subgraph cluster_1 {
style="rounded,dashed"; penwidth = 3; color = "black";
label = "Computer 1"; labelloc=b;
fontname="Lato"; fontsize="40pt"; fontcolor = "black";
node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", 
      fillcolor = "#C3FFEC", margin = .25]
LR1 [label = "OurProject\n(Local Repository)"]
}

subgraph cluster_2 {
style="rounded,dashed"; penwidth = 3; color = "black";
label = "Computer 2"; labelloc=b;
fontname="Lato"; fontsize="40pt"; fontcolor = "black";
node [shape = "box", style= "filled", fontname="Lato", fontsize="40pt", 
      fillcolor = "#C3FFEC", margin = .25]
LR2 [label = "OurProject\n(Local Repository)"]
}

edge [fontname="Lato", fontsize="30pt", arrowsize=1.5]
RR:s -> Hidden:n [style = "invis"]
Hidden:w -> LR1:ne [style = "invis"]
Hidden:e -> LR2:nw [style = "invis"]
RR -> LR1 [headlabel = "Clone/\rPull\r", labeldistance = 6, labelangle = 45]
RR -> LR1:ne [taillabel = "Push\l", dir = back, labeldistance = 5, labelangle = 45]
RR -> LR2:nw [taillabel = "Push\r", dir = back, labeldistance = 5, labelangle = -45]
RR -> LR2 [headlabel = "Clone/\lPull\l", labeldistance = 6, labelangle = -45]
}
```

::: {.notes}
GitHub is an online server that hosts Git repositories, allowing multiple people
to clone copies from the remote server to their own computers and synchronize
their local copies with the main one hosted on GitHub. This facilitates
collaboration because each person can independently edit files in their own
local copy, pull the recent changes others have made down from GitHub, and push
their own changes back up to the main repository. Git will detect conflicting
changes and force a user to resolve them and commit the result.

Using a private GitHub repository while working on the project allows you 
collaborate with teammates as you develop a report. Later, you can make the 
repository public so that people outside your team can access reproducibility 
materials. 
:::

## Collaboration

Treat RR as a form of collaborating with multiple stakeholders. Give them clear, 
useful, and relevant materials. 

Who are your key collaborators?

* Your future self
* Your current team
* Peer-reviewers
* Your audience

Add diagram / photo illustrating collaboration with self, team-mates, and readers

::: {.notes}
Plan your work as a collaborative effort. Knowing you will share materials with
other people orients you to the need to communicate what you are doing and how
the materials you have developed can be used to reproduce results. It motivates
you to make things clear and useful. 
:::

## Organization

* Organize files into a research compendium.
* Create documentation (README files, vignettes, etc.). 
* Use folder structures, naming conventions, 
* Use data flow diagrams and/or rendering scripts

::: {.notes}
Git repository + RStudio project + R package structure + Quarto project
Standardizing across projects is useful for efficiency
:::



## Automation

* Use scripts instead of GUIs
* Make computers do the tedious labor
* Computers are patient idiots


::: {.notes}
Add speaker notes here.
:::


## Preservation
What should you preserve?

* Data and meta-data
* Methodology decisions
* Data cleaning, management, and analysis scripts
* Key decisions and rationales
* Source code for scripts
* Version histories of data, code, output, and deliverables
* Who changed what & when
* Software environment & versions used

::: {.notes}
Add speaker notes here.
:::


## Integration
* Attach meta-data directly to data 
* Include codebooks (or scripts to generate them from the data)
* Integrate narrative text with code via Markup languages 
* Add custom functions and associated help files to research compendium
* Include references 

::: {.notes}
Variable & value labels. 
README files & other documentatioon
Quarto!
:::

## Separation

* Data from code
* Raw data from cleaned data
* Code from output
* Draft from production output
* Version history from current files

::: {.notes}
Add speaker notes here.
:::

## Version Control 
* Git tracks how files changed (additions & deletions), who changed them, when, & why
* Searchable & recoverable version history
* Eliminates the need to use filename conventions for versioning

::: {.notes}
Speaker notes go here.
:::

## Narrative Text

* Formatting
* Headings
* Links

## Inline R Code


## Tables


## Figures


## Parameterized Reports


## Map Your Path to Reproducibility {background-image="graphics/Map_Compass_Navigation.jpg" background-size="contain" background-position="right" background-repeat="no-repeat"}



## References {.scrollable}

::: {#refs}
:::

